worker_processes  {{ .Config.WorkerProcesses }};
daemon off;

error_log stderr {{ .Config.LogLevel }};
pid {{ .Config.WorkingDir }}/nginx.pid;

events {
    # Accept connections as fast as possible.
    multi_accept on;
    # Includes both proxy and client connections.
    # So e.g. 4096 = 2048 persistent client connections to backends per worker.
    worker_connections {{ .Config.WorkerConnections }};
    # Use most optimal non-blocking selector on linux.
    # Should be selected by default on linux, we just make it explicit here.
    use epoll;
}

http {
    default_type text/html;

    # Keep alive time for client connections. Don't limit by number of requests.
    keepalive_timeout {{ .Config.KeepaliveSeconds }}s;
    keepalive_requests 2147483647;

    # Optimize for latency over throughput for persistent connections.
    tcp_nodelay on;

    # Disable nginx version leakage to external clients.
    server_tokens off;

    # Obtain client IP from frontend's X-Forward-For header
    set_real_ip_from 0.0.0.0/0;
    real_ip_header X-Forwarded-For;
    real_ip_recursive off;

    # Put all data into the working directory.
    access_log             off;
    client_body_temp_path  {{ .Config.WorkingDir }}/tmp_client_body 1 2;
    proxy_temp_path        {{ .Config.WorkingDir }}/tmp_proxy 1 2;
    fastcgi_temp_path      {{ .Config.WorkingDir }}/tmp_fastcgi 1 2;
    uwsgi_temp_path        {{ .Config.WorkingDir }}/tmp_uwsgi 1 2;
    scgi_temp_path         {{ .Config.WorkingDir }}/tmp_scgi 1 2;

    # Configure ingresses
    {{ $port := .Config.IngressPort }}
    {{ $keepalive := .Config.BackendKeepalives }}
    {{ $keepaliveSeconds := .Config.BackendKeepaliveSeconds }}
    {{ range $entry := .Entries }}
    # Start entry
    # {{ $entry.Name }}
    upstream {{ $entry.UpstreamID }} {
        server {{ $entry.ServiceAddress }}:{{ $entry.ServicePort }};
        keepalive {{ $keepalive }};
    }

    server {
        listen {{ $port }};
        server_name {{ $entry.Host }};

        # Restrict clients
        allow 127.0.0.1;
        {{ range $entry.Allow }}allow {{ . }};
        {{ end }}
        deny all;

        location {{ if $entry.Path }}{{ $entry.Path }}{{ end }} {
            # Strip location path when proxying.
            proxy_pass http://{{ $entry.UpstreamID }}/;

            # Enable keepalive to backend.
            proxy_http_version 1.1;
            proxy_set_header Connection "";

            # Add X-Forwarded-For and X-Original-URI for proxy information.
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Original-URI $request_uri;

            # Timeout faster than the default 60s on initial connect.
            proxy_connect_timeout 10s;

            # Close proxy connections after backend keepalive time.
            proxy_read_timeout {{ $keepaliveSeconds }}s;
            proxy_send_timeout {{ $keepaliveSeconds }}s;

            # Disable buffering, as we'll be interacting with ELBs with http listeners, which we assume will
            # quickly consume and generate responses and requests.
            # This should be enabled if nginx will directly serve traffic externally to unknown clients.
            proxy_buffering off;
            proxy_request_buffering off;
        }
    }
    # End entry
    {{ end }}

    # End ingresses

    # Default backend
    server {
        listen {{ .Config.IngressPort }} default_server;
        location / {
            return 404;
        }
    }

    # Status port. This should be firewalled to only allow internal access.
    server {
        listen {{ .Config.HealthPort }} default_server reuseport;

        location /health {
            access_log off;
            return 200;
        }

        location /status {
            access_log off;
            stub_status;
        }

        location / {
            return 404;
        }
    }
}
